{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "568ed223-4afb-4950-89d8-fbe97c8c5beb",
   "metadata": {},
   "source": [
    "# Summary of the Feature Selection and Modeling Process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0fb23b-c2e4-4338-b827-31ecd3da677e",
   "metadata": {},
   "source": [
    "This work aims to predict the fare amount of yellow taxi trips in New York City using a machine learning approach based on systematic feature selection and model comparison.\n",
    "After cleaning the dataset and removing variables that directly reconstruct the fare (e.g., total_amount, tip_amount), several feature-selection techniques were applied:\n",
    "\n",
    "ANOVA F-test (linear dependency)\n",
    "\n",
    "Mutual Information Regression (non-linear dependency)\n",
    "\n",
    "Random Forest Feature Importances\n",
    "\n",
    "Gradient Boosting Feature Importances\n",
    "\n",
    "All four methods consistently identified the same core explanatory variables, with trip_distance being by far the most influential predictor, followed by RateCodeID, geographic coordinates (pickup/dropoff), and payment_type.\n",
    "Tree-based models provided the highest predictive accuracy (R² ≈ 0.93–0.94), and the Random Forest model showed excellent robustness and interpretability.\n",
    "\n",
    "Based on the consistency of results across methods and the strong performance of ensemble models, our group selected Random Forest as the final predictive model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "881b37f4-31ce-4df8-a1d4-f6d00841fefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "697c8c15-cd0c-4bf9-a5da-64328c5c1fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>VendorID</th>\n",
       "      <th>tpep_pickup_datetime</th>\n",
       "      <th>tpep_dropoff_datetime</th>\n",
       "      <th>passenger_count</th>\n",
       "      <th>trip_distance</th>\n",
       "      <th>pickup_longitude</th>\n",
       "      <th>pickup_latitude</th>\n",
       "      <th>RateCodeID</th>\n",
       "      <th>store_and_fwd_flag</th>\n",
       "      <th>dropoff_longitude</th>\n",
       "      <th>dropoff_latitude</th>\n",
       "      <th>payment_type</th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>extra</th>\n",
       "      <th>mta_tax</th>\n",
       "      <th>tip_amount</th>\n",
       "      <th>tolls_amount</th>\n",
       "      <th>improvement_surcharge</th>\n",
       "      <th>total_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-01 16:07:13</td>\n",
       "      <td>2015-01-01 16:35:55</td>\n",
       "      <td>1</td>\n",
       "      <td>18.26</td>\n",
       "      <td>-73.979401</td>\n",
       "      <td>40.760380</td>\n",
       "      <td>3</td>\n",
       "      <td>N</td>\n",
       "      <td>-74.176826</td>\n",
       "      <td>40.694511</td>\n",
       "      <td>2</td>\n",
       "      <td>66.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.35</td>\n",
       "      <td>0.3</td>\n",
       "      <td>82.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-01 15:59:47</td>\n",
       "      <td>2015-01-01 16:02:06</td>\n",
       "      <td>1</td>\n",
       "      <td>0.66</td>\n",
       "      <td>-73.980637</td>\n",
       "      <td>40.730099</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.983124</td>\n",
       "      <td>40.722569</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-01 04:05:37</td>\n",
       "      <td>2015-01-01 04:08:25</td>\n",
       "      <td>5</td>\n",
       "      <td>0.33</td>\n",
       "      <td>-73.971985</td>\n",
       "      <td>40.749748</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.971886</td>\n",
       "      <td>40.746304</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.3</td>\n",
       "      <td>5.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 10:10:53</td>\n",
       "      <td>2015-01-01 10:27:01</td>\n",
       "      <td>3</td>\n",
       "      <td>7.30</td>\n",
       "      <td>-74.010773</td>\n",
       "      <td>40.714050</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.950073</td>\n",
       "      <td>40.779720</td>\n",
       "      <td>1</td>\n",
       "      <td>22.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>28.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 03:30:14</td>\n",
       "      <td>2015-01-01 03:46:46</td>\n",
       "      <td>1</td>\n",
       "      <td>2.60</td>\n",
       "      <td>-73.982277</td>\n",
       "      <td>40.742973</td>\n",
       "      <td>1</td>\n",
       "      <td>N</td>\n",
       "      <td>-73.990211</td>\n",
       "      <td>40.712654</td>\n",
       "      <td>2</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   VendorID tpep_pickup_datetime tpep_dropoff_datetime  passenger_count  \\\n",
       "0         2  2015-01-01 16:07:13   2015-01-01 16:35:55                1   \n",
       "1         2  2015-01-01 15:59:47   2015-01-01 16:02:06                1   \n",
       "2         2  2015-01-01 04:05:37   2015-01-01 04:08:25                5   \n",
       "3         1  2015-01-01 10:10:53   2015-01-01 10:27:01                3   \n",
       "4         1  2015-01-01 03:30:14   2015-01-01 03:46:46                1   \n",
       "\n",
       "   trip_distance  pickup_longitude  pickup_latitude  RateCodeID  \\\n",
       "0          18.26        -73.979401        40.760380           3   \n",
       "1           0.66        -73.980637        40.730099           1   \n",
       "2           0.33        -73.971985        40.749748           1   \n",
       "3           7.30        -74.010773        40.714050           1   \n",
       "4           2.60        -73.982277        40.742973           1   \n",
       "\n",
       "  store_and_fwd_flag  dropoff_longitude  dropoff_latitude  payment_type  \\\n",
       "0                  N         -74.176826         40.694511             2   \n",
       "1                  N         -73.983124         40.722569             2   \n",
       "2                  N         -73.971886         40.746304             2   \n",
       "3                  N         -73.950073         40.779720             1   \n",
       "4                  N         -73.990211         40.712654             2   \n",
       "\n",
       "   fare_amount  extra  mta_tax  tip_amount  tolls_amount  \\\n",
       "0         66.0    0.0      0.0         0.0         16.35   \n",
       "1          4.0    0.0      0.5         0.0          0.00   \n",
       "2          4.0    0.5      0.5         0.0          0.00   \n",
       "3         22.5    0.0      0.5         5.0          0.00   \n",
       "4         12.5    0.5      0.5         0.0          0.00   \n",
       "\n",
       "   improvement_surcharge  total_amount  \n",
       "0                    0.3         82.65  \n",
       "1                    0.3          4.80  \n",
       "2                    0.3          5.30  \n",
       "3                    0.0         28.30  \n",
       "4                    0.0         13.80  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import dataset\n",
    "df = pd.read_csv(r'df_sample.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8978218a-d5fd-4192-b215-d6d9c537a1c6",
   "metadata": {},
   "source": [
    "## Preprocessing Setup: Numerical and Categorical Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df29a18-e85d-4303-8077-20c766cda882",
   "metadata": {},
   "source": [
    "Before applying any feature-selection method, we define a preprocessing pipeline that standardizes numerical features and one-hot encodes categorical features.\n",
    "This ensures that all downstream feature-selection techniques and machine-learning models operate on transformed, comparable data.\n",
    "The ColumnTransformer automatically applies the right transformation to each column type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9a9754d-8a67-4bc8-91ea-cc55f55dbdb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Performances ===\n",
      "MAE : 1.77\n",
      "RMSE : 4.03\n",
      "R² : 0.8424\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "# === 1. Colonne cible ===\n",
    "target = \"fare_amount\"   # ou celle que tu veux\n",
    "\n",
    "# === 2. Séparation X / y ===\n",
    "cols_to_remove = [\n",
    "    'total_amount',\n",
    "    'tip_amount',\n",
    "    'tolls_amount',\n",
    "    'extra',\n",
    "    'mta_tax',\n",
    "    'improvement_surcharge'\n",
    "]\n",
    "X = df.drop(columns=[target] + cols_to_remove)\n",
    "y = df[target]\n",
    "\n",
    "# === 3. Train/test ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# === 4. Sélection automatique des types ===\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# === 5. Préprocesseur : Imputation + Scaling + Encoding ===\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# === 6. Pipeline complet ===\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# === 7. Entraînement ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# === 8. Prédictions ===\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# === 9. Métriques ===\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"=== Performances ===\")\n",
    "print(f\"MAE : {mae:.2f}\")\n",
    "print(f\"RMSE : {rmse:.2f}\")\n",
    "print(f\"R² : {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebadcd48-64eb-4fff-9a58-a189fd8a2303",
   "metadata": {},
   "source": [
    "## Feature Selection using ANOVA F-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd57e54-256d-4854-9b1c-411c2931cd80",
   "metadata": {},
   "source": [
    "In this section, we apply a linear feature-selection technique (ANOVA F-score) to evaluate how strongly each input variable is linearly associated with the target (fare_amount).\n",
    "The SelectKBest method ranks all features by F-score and allows us to print the sorted ranking.\n",
    "This provides an interpretable, variance-based baseline for feature relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4f78977c-33f8-41fe-9916-7eba71f745a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      Feature        F-score\n",
      "2          num__trip_distance  353996.057984\n",
      "5             num__RateCodeID    6988.961014\n",
      "8           num__payment_type     593.938775\n",
      "3       num__pickup_longitude      42.392855\n",
      "6      num__dropoff_longitude      41.727704\n",
      "7       num__dropoff_latitude      41.265916\n",
      "4        num__pickup_latitude      41.257718\n",
      "0               num__VendorID      13.568280\n",
      "1        num__passenger_count       7.533220\n",
      "10  cat__store_and_fwd_flag_Y       2.568141\n",
      "9   cat__store_and_fwd_flag_N       2.568141\n",
      "\n",
      "Features sélectionnées : ['num__trip_distance', 'num__RateCodeID', 'num__payment_type', 'num__pickup_longitude', 'num__dropoff_longitude', 'num__dropoff_latitude', 'num__pickup_latitude', 'num__VendorID', 'num__passenger_count', 'cat__store_and_fwd_flag_Y']\n",
      "\n",
      "MAE: 1.7477\n",
      "RMSE: 3.9604\n",
      "R²:   0.8481\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === Colonnes déjà présentes, plus aucune datetime ===\n",
    "target = \"fare_amount\"\n",
    "\n",
    "cols_to_remove = [\n",
    "    'total_amount','tip_amount','tolls_amount',\n",
    "    'extra','mta_tax','improvement_surcharge'\n",
    "]\n",
    "\n",
    "df = df.drop(columns=cols_to_remove)\n",
    "\n",
    "# === Reconstruction X, y ===\n",
    "X = df.drop(columns=[target])\n",
    "y = df[target]\n",
    "\n",
    "# === Séparer num / cat ===\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# === Préprocesseur ===\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numeric_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# === ANOVA ===\n",
    "K = 10\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"selector\", SelectKBest(score_func=f_regression, k=K)),\n",
    "    (\"regressor\", LinearRegression())\n",
    "])\n",
    "\n",
    "# === Split ===\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# === Fit ===\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# === Extraction scores ===\n",
    "selector = pipeline.named_steps['selector']\n",
    "scores = selector.scores_\n",
    "\n",
    "encoded_features = pipeline.named_steps['preprocessor'].get_feature_names_out()\n",
    "\n",
    "scores_df = pd.DataFrame({'Feature': encoded_features, 'F-score': scores})\n",
    "scores_df = scores_df.sort_values('F-score', ascending=False)\n",
    "print(scores_df)\n",
    "\n",
    "selected_features = scores_df.head(K)['Feature'].tolist()\n",
    "print(\"\\nFeatures sélectionnées :\", selected_features)\n",
    "\n",
    "# === Prédiction ===\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "# === Métriques ===\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nMAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²:   {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22053b4-0684-4c89-926f-2d19a2b2638a",
   "metadata": {},
   "source": [
    " ## Feature Selection using Mutual Information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e061ccc-5fce-47a6-926d-51cb581c4245",
   "metadata": {},
   "source": [
    "Mutual Information captures non-linear dependencies between each feature and the target variable.\n",
    "This method does not rely on linear assumptions and can identify complex relationships that ANOVA may miss.\n",
    "We compute MI scores, sort all features by importance, and extract the top contributors.\n",
    "Comparing MI with ANOVA helps validate the stability of feature relevance across methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59b46c33-40a1-47e0-b83f-e1746c6275dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mutual Information Scores triés par ordre décroissant:\n",
      "                Feature  MI_Score\n",
      "2         trip_distance  1.396470\n",
      "7      dropoff_latitude  0.118202\n",
      "6     dropoff_longitude  0.114360\n",
      "3      pickup_longitude  0.114099\n",
      "5            RateCodeID  0.107425\n",
      "4       pickup_latitude  0.092537\n",
      "8          payment_type  0.013859\n",
      "0              VendorID  0.003326\n",
      "9  store_and_fwd_flag_Y  0.000417\n",
      "1       passenger_count  0.000000\n",
      "\n",
      "Top 10 features sélectionnées:\n",
      "['trip_distance', 'dropoff_latitude', 'dropoff_longitude', 'pickup_longitude', 'RateCodeID', 'pickup_latitude', 'payment_type', 'VendorID', 'store_and_fwd_flag_Y', 'passenger_count']\n",
      "\n",
      "MAE: 1.7477\n",
      "RMSE: 3.9604\n",
      "R²: 0.8481\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "X = df.drop(columns=['fare_amount'])\n",
    "y = df['fare_amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features)\n",
    "    ])\n",
    "\n",
    "X_train_transformed = preprocessor.fit_transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "feature_names_num = numeric_features\n",
    "feature_names_cat = []\n",
    "if len(categorical_features) > 0:\n",
    "    feature_names_cat = preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
    "feature_names = feature_names_num + feature_names_cat\n",
    "\n",
    "mi_scores = mutual_info_regression(X_train_transformed, y_train, random_state=42)\n",
    "\n",
    "mi_df = pd.DataFrame({'Feature': feature_names, 'MI_Score': mi_scores})\n",
    "mi_df = mi_df.sort_values('MI_Score', ascending=False)\n",
    "\n",
    "print(\"Mutual Information Scores triés par ordre décroissant:\")\n",
    "print(mi_df)\n",
    "\n",
    "K = 10\n",
    "top_k_features = mi_df.head(K)['Feature'].tolist()\n",
    "top_k_indices = [feature_names.index(f) for f in top_k_features]\n",
    "\n",
    "X_train_selected = X_train_transformed[:, top_k_indices]\n",
    "X_test_selected = X_test_transformed[:, top_k_indices]\n",
    "\n",
    "print(f\"\\nTop {K} features sélectionnées:\")\n",
    "print(top_k_features)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_selected, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test_selected)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nMAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71793660-262c-4a02-bff5-69bc42c51005",
   "metadata": {},
   "source": [
    "## Random Forest Regressor and Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72994ed-bb16-4d1a-a533-3c2f9a7c49e5",
   "metadata": {},
   "source": [
    "Here we train a Random Forest model using the preprocessing pipeline.\n",
    "Tree-based models naturally compute feature importances based on how frequently each variable is used to improve decision splits.\n",
    "This method is robust to noise and non-linearity, and provides a realistic estimate of which variables matter most for prediction.\n",
    "We evaluate model performance (MAE, RMSE, R²) and display the ranked feature importances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58c18c94-d980-4efa-8188-349bbbb98a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances triées par ordre décroissant:\n",
      "                Feature  Importance\n",
      "2         trip_distance    0.866813\n",
      "5            RateCodeID    0.069846\n",
      "6     dropoff_longitude    0.014733\n",
      "3      pickup_longitude    0.014373\n",
      "4       pickup_latitude    0.013035\n",
      "7      dropoff_latitude    0.012604\n",
      "8          payment_type    0.004378\n",
      "1       passenger_count    0.002479\n",
      "0              VendorID    0.001465\n",
      "9  store_and_fwd_flag_Y    0.000274\n",
      "\n",
      "Top 10 features les plus importantes:\n",
      "['trip_distance', 'RateCodeID', 'dropoff_longitude', 'pickup_longitude', 'pickup_latitude', 'dropoff_latitude', 'payment_type', 'passenger_count', 'VendorID', 'store_and_fwd_flag_Y']\n",
      "\n",
      "MAE: 1.2348\n",
      "RMSE: 2.5978\n",
      "R²: 0.9346\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "X = df.drop(columns=['fare_amount'])\n",
    "y = df['fare_amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features)\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "feature_names_num = numeric_features\n",
    "feature_names_cat = []\n",
    "if len(categorical_features) > 0:\n",
    "    feature_names_cat = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
    "feature_names = feature_names_num + feature_names_cat\n",
    "\n",
    "importances = pipeline.named_steps['regressor'].feature_importances_\n",
    "\n",
    "importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "importances_df = importances_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importances triées par ordre décroissant:\")\n",
    "print(importances_df)\n",
    "\n",
    "top_10_features = importances_df.head(10)['Feature'].tolist()\n",
    "print(f\"\\nTop 10 features les plus importantes:\")\n",
    "print(top_10_features)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nMAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db0103-d8ac-456c-9e9d-1e21ae9b606f",
   "metadata": {},
   "source": [
    "## Gradient Boosting Regressor and Feature Importances"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d25d68a-cc5a-44b9-84ed-2a107001dd83",
   "metadata": {},
   "source": [
    "Gradient Boosting is another ensemble method that builds sequential decision trees to reduce prediction error.\n",
    "It provides smoother importance scores and handles complex relationships efficiently.\n",
    "We train a Gradient Boosting model with the same preprocessing pipeline and display the feature importances and evaluation metrics.\n",
    "This allows us to check the consistency of results across different ensemble models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ed4ea95f-2e16-481c-8035-87344c51d409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances triées par ordre décroissant:\n",
      "                Feature  Importance\n",
      "2         trip_distance    0.903787\n",
      "5            RateCodeID    0.074444\n",
      "6     dropoff_longitude    0.008543\n",
      "7      dropoff_latitude    0.005017\n",
      "3      pickup_longitude    0.002760\n",
      "4       pickup_latitude    0.002671\n",
      "8          payment_type    0.002337\n",
      "0              VendorID    0.000222\n",
      "9  store_and_fwd_flag_Y    0.000124\n",
      "1       passenger_count    0.000095\n",
      "\n",
      "Top 10 features les plus importantes:\n",
      "['trip_distance', 'RateCodeID', 'dropoff_longitude', 'dropoff_latitude', 'pickup_longitude', 'pickup_latitude', 'payment_type', 'VendorID', 'store_and_fwd_flag_Y', 'passenger_count']\n",
      "\n",
      "MAE: 1.2956\n",
      "RMSE: 2.6426\n",
      "R²: 0.9324\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "X = df.drop(columns=['fare_amount'])\n",
    "y = df['fare_amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "numeric_features = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numeric_features),\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features)\n",
    "    ])\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', GradientBoostingRegressor(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "feature_names_num = numeric_features\n",
    "feature_names_cat = []\n",
    "if len(categorical_features) > 0:\n",
    "    feature_names_cat = pipeline.named_steps['preprocessor'].named_transformers_['cat'].get_feature_names_out(categorical_features).tolist()\n",
    "feature_names = feature_names_num + feature_names_cat\n",
    "\n",
    "importances = pipeline.named_steps['regressor'].feature_importances_\n",
    "\n",
    "importances_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "importances_df = importances_df.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importances triées par ordre décroissant:\")\n",
    "print(importances_df)\n",
    "\n",
    "top_10_features = importances_df.head(10)['Feature'].tolist()\n",
    "print(f\"\\nTop 10 features les plus importantes:\")\n",
    "print(top_10_features)\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nMAE: {mae:.4f}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"R²: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650d19fb-f9e5-42dd-9302-1f0270e4e012",
   "metadata": {},
   "source": [
    "## Cross-Method Comparison and Final Feature Assessment\n",
    "After evaluating ANOVA, Mutual Information, Random Forest, and Gradient Boosting, we compare the rankings obtained from each method.\n",
    "We confirm that the same core set of features consistently appears as the most influential, especially trip_distance, RateCodeID, and the geographical coordinates.\n",
    "This validation ensures strong confidence in the selected variables.\n",
    "The team ultimately chose Random Forest as the final model due to its excellent predictive performance, robustness, and interpretability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
